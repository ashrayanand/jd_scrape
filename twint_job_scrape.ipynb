{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('twint': conda)",
   "metadata": {
    "interpreter": {
     "hash": "57b3856b3ab093e5a17f784ece241abfc76a38521d7264e7d1d29b0b3abe1c87"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/Ash/miniconda3/envs/twint/bin/python\n"
     ]
    }
   ],
   "source": [
    "# verifying virtual environment\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project imports\n",
    "import twint\n",
    "import pandas as pd\n",
    "\n",
    "# !pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "source": [
    "Working details: Currently, twint is only working with Python 3.8+ and requires nest_asyncio. Details found after a handful of online digging. Quite lucky for it to still be working after the twitter 2020 crackdown on scraping and monetising the scraping."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a silencer function\n",
    "def silent_execute(fn):\n",
    "    '''\n",
    "    To silently execute a function\n",
    "    '''\n",
    "    old_stdout = sys.stdout  # backup current stdout\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    fn\n",
    "    sys.stdout = old_stdout  # reset old stdout\n",
    "    return 0\n",
    "\n",
    "# for an IPython notebook simply use %%capture in the cell\n",
    "# this function will be useful sometime later"
   ]
  },
  {
   "source": [
    "%%capture\n",
    "\n",
    "\n",
    "# keywords_list = ['datascience', 'data', 'artificial intelligence', 'job', 'recruit', 'engineer', 'scientist', 'deep learning', 'computer vision', 'reinforcement learning', 'natural language processing', 'hiring', 'hire', 'recruiting', 'nlp', 'technology', 'researcher', 'engineer', 'science', 'ai', 'ml', 'tech']\n",
    "# twitter has a keyword limit\n",
    "\n",
    "keywords_list = ['job','recruit', 'position', 'open', 'scientist', 'researcher', 'hire', 'hiring', 'work', 'remote', 'lab', 'university', 'engineer']\n",
    "\n",
    "search_str = \"\"\n",
    "for i in keywords_list:\n",
    "    if i == keywords_list[0]:\n",
    "        search_str = search_str + i\n",
    "    else:\n",
    "        search_str = search_str + \" OR \" + i\n",
    "print(search_str)\n",
    "\n",
    "\n",
    "# okay let's bring it\n",
    "c = twint.Config()\n",
    "c.Lang = \"en\"\n",
    "c.Search = search_str  # keywords\n",
    "\n",
    "c.Limit = 1000000\n",
    "# twitter has a limit(~10000) on scraping but this can be easily by-passed by sleeping and restarting,\n",
    "# switching IP, shuffling whatever identifier mechanism is used and so on\n",
    "# as a proof of concept, 10k is sufficient\n",
    "\n",
    "# sampling date wise is also cool - economical and efficient\n",
    "# c.Since = \"2020-11-29 00:00:00\"\n",
    "# c.Until = \"2020-11-30 00:05:00\"\n",
    "\n",
    "c.Pandas = True\n",
    "\n",
    "twint.run.Search(c)\n",
    "Tweets_df = twint.storage.panda.Tweets_df\n",
    "Tweets_df.to_csv('data_2.csv')\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10000\n0.362396240234375 MB\n"
     ]
    }
   ],
   "source": [
    "# print(Tweets_df.columns)\n",
    "print(len(Tweets_df))\n",
    "print(Tweets_df.size / (2 ** 20), \"MB\")\n",
    "# print(Tweets_df.iloc[10])\n",
    "# for j in range(10, 20):\n",
    "#     print(Tweets_df['tweet'].iloc[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.3719329833984375 MB\nIndex(['Unnamed: 0', 'id', 'conversation_id', 'created_at', 'date', 'timezone',\n       'place', 'tweet', 'language', 'hashtags', 'cashtags', 'user_id',\n       'user_id_str', 'username', 'name', 'day', 'hour', 'link', 'urls',\n       'photos', 'video', 'thumbnail', 'retweet', 'nlikes', 'nreplies',\n       'nretweets', 'quote_url', 'search', 'near', 'geo', 'source',\n       'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date',\n       'translate', 'trans_src', 'trans_dest'],\n      dtype='object')\n10000\n"
     ]
    }
   ],
   "source": [
    "# reloading db\n",
    "LOADED_DF = pd.read_csv('data_1.csv')\n",
    "\n",
    "print(LOADED_DF.size / (2 ** 20), \"MB\")\n",
    "print(LOADED_DF.columns)\n",
    "print(len(LOADED_DF))\n",
    "# print(LOADED_DF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Good morning everyone! Iâ€™m so blessed to be starting work at 11 today. Had the job my second week Iâ€™m so happy!\n@OklianaFarms @CoBoJo_ Does look great. I am envious of those who can do this work. A hammer in my hand is an implement of destruction and chaos.\nWant to work at Rain Bird? We're hiring in Tucson, AZ! Click the link in our bio for details on this job and more: Manufacturing Manager #Manufacturing #Extrusion\n@creamymars ðŸ¥ºðŸ¥ºðŸ¥º\nFuck nigga u a worker do yo job remember we ainâ€™t friends\nIf he wasnâ€™t at work... chile we wouldâ€™ve been in here carrying on ðŸ˜©\nI cannot wait to take a map after work\n@Pszych244 @DNAD23 @MarkClifford86 @geoffkeene He wasnâ€™t actually, this was supposed to be volunteer work and yet he decided to bring up money and basically try to take a shortcut. Not only that but he was doing this just to try and make himself look good for the other jobs he was applying for.\nWant to work in #Camden, NJ? View our latest opening:  https://t.co/aoX7fnf1ND\n@bennyjohnson Are they so stupid they can't see lockdowns DO NOT work.\n@paprikapear Hello! My name is Matt/Hobs and I am a ttrpg enthusiast who loves to create. Trying to get into the commission biz as I am a full time parent by day, so these artshares are wonderful. Thank you so much!!  https://t.co/WWM7ertU0c\n@TweetyKnowles @4XsACharm @brianglicklich @ScottBaio @NicoleEggert I would welcome it too! Nicole has witnesses and polygraph tests....BRING THEM BOTH FORWARD. If she has evidence to back up her claims, let it go before the media and court of law. That's how things work! SOL can be ignored at the DA's discretion. Innocent till proven guilty.\nReally feel like making one excuse to not go to work.....#BedFeelsSoNice\nWhen you compare yourself with others in matters of wealth, position, and health, you should look at people less favoured than yourself. When you compare yourself with others in matters of religion, knowledge and virtue, look at people who are better than yourself. âœ¨âœ¨âœ¨\nDon't be complacent. If we lose because of inconsistency then regret will be eating us from inside. But you're bit in this alone. We are together in this. We'll do a good job\nJoin the Dollar Tree team! See our latest #Sales job openings, including \"SALES FLOOR ASSOCIATE\", via the link in our bio. #Ashburn, VA\nEverdine  https://t.co/Xf2anz6Bjw\nSo youâ€™re complaining that some restaurants are staying open yet you work in retail. Why should you be able to work and support yourself but they canâ€™t? ðŸ™„\n@TheBrandonKang @beeple Yes of course, although being well known doesnâ€™t always equal immediate sales and value prop (re: all dead artists whoâ€™s work is $$$ now than before).   Secondary markets can be healthy if the artist benefits and because this drop is LARGE enough where itâ€™s not that scarce.\n@Fulwell_End Fucking madness.\n"
     ]
    }
   ],
   "source": [
    "# exploring db and showing samples\n",
    "for j in range (100, 120):\n",
    "    print(LOADED_DF['tweet'].iloc[j])\n",
    "\n",
    "# as we can see some tweets are about jobs and some are irrelevant, and this raw data can be further processed for # proper querying, relevance ordering of results, location filtering and so forth"
   ]
  }
 ]
}